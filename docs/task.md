# НТО 2025/2026 — Профиль «Искусственный интеллект»

## Этап 2Б: Ранжирование прочитанных книг

---

## 1. Описание задачи

**Контекст:** Книжная платформа различает два типа пользовательской активности: добавление книги в «планы» (`has_read=0`) и фактическое прочтение (`has_read=1`). Это сигналы разной силы: намерение прочесть не всегда приводит к реальному действию.

**Цель:** Разработать модель, которая сможет отличить сильный сигнал (факт прочтения) от слабого (намерение). Для каждого пользователя необходимо ранжировать предложенный список книг-кандидатов таким образом, чтобы книги, которые он действительно прочитал (но которые были скрыты из обучающих данных), оказались как можно выше в списке.

Это задача Learning-to-Rank (LTR), проверяющая навыки построения моделей, способных выявлять скрытые закономерности, ведущие к конверсии из интереса в фактическое потребление контента.

### 1.1. Особенности задачи

**Временное разделение:** Обучающая выборка (`train.csv`) содержит историю взаимодействий пользователей до определенного момента времени (T_global), а тестовые данные (кандидаты для ранжирования) относятся к более позднему периоду. Это обеспечивает реалистичность задачи: участники должны предсказать будущие взаимодействия на основе прошлых.

**Реалистичные кандидаты:** Для каждого пользователя в `candidates.csv` предоставляются все книги, с которыми он взаимодействовал в тестовом периоде (и прочитанные, и запланированные). Это означает, что участники ранжируют реальные взаимодействия пользователя, а не случайный набор книг.

**Временные метки:** В обучающей выборке (`train.csv`) присутствуют временные метки (`timestamp`), что позволяет участникам строить временные модели и анализировать паттерны взаимодействий. Однако в `candidates.csv` временные метки не предоставляются, что требует использования других признаков для ранжирования и усложняет задачу.

**Изоляция данных:** Пользователи Stage 2 полностью изолированы от пользователей Stage 1, что гарантирует независимость этапов соревнования.

---

## 2. Данные

Участникам предоставляется набор файлов, содержащий историю взаимодействий, пулы кандидатов для ранжирования и необходимые метаданные.

**Основные файлы:**
- `train.csv` — обучающая история взаимодействий (с временными метками)
- `targets.csv` — список пользователей для ранжирования
- `candidates.csv` — пулы кандидатов для каждого пользователя (без временных меток)
- `books.csv`, `users.csv`, `genres.csv`, `book_genres.csv`, `book_descriptions.csv` — метаданные

> **Подробное описание** состава файлов, структуры таблиц, полей и принципов формирования данных вынесено в сопроводительный документ: «Этап 2Б: Описание данных и формата решений».

---

## 3. Формат решения

Файл с решением должен быть представлен в формате CSV с разделителем запятая (`,`).

> **Подробные требования** к структуре колонок, типам данных и ограничениям описаны в сопроводительном документе: «Этап 2Б: Описание данных и формата решений».

---

## 4. Метрика оценки

Итоговый балл рассчитывается как взвешенная сумма двух стандартных метрик для задач ранжирования: NDCG@20 (качество упорядочивания) и Recall@20 (полнота).

### 4.1. NDCG@20 (Normalized Discounted Cumulative Gain)

Оценивает, насколько высоко в предложенном ранжированном списке (до 20 позиций) находятся релевантные (прочитанные пользователем) книги. Метрика учитывает порядок: чем выше релевантный элемент, тем больше вклад. Если список короче 20 позиций, метрика рассчитывается по всем доступным позициям.

Для каждого пользователя $u$ рассчитывается:

$$\mathrm{DCG@20}(u) = \sum_{i=1}^{\min(|R(u)|, 20)} \frac{rel(i)}{\log_2(i+1)}$$

$$\mathrm{IDCG@20}(u) = \sum_{i=1}^{\min(K, 20)} \frac{1}{\log_2(i+1)}$$

$$\mathrm{NDCG@20}(u) = \frac{\mathrm{DCG@20}(u)}{\mathrm{IDCG@20}(u)}$$

где $|R(u)|$ — длина ранжированного списка для пользователя $u$ (если список короче 20, суммирование идет по всем доступным позициям).

Итоговая метрика является средним значением по всем пользователям:

$$\overline{\mathrm{NDCG@20}} = \frac{1}{N} \sum_{j=1}^N \mathrm{NDCG@20}(u_j)$$

**Обработка edge case (пустой solution):** Если пользователь ничего не прочитал в тестовом периоде ($K = 0$), то стандартные формулы дают деление на ноль (IDCG@20 = 0). Для таких случаев применяется специальная логика:
- Если участник отправил пустой список ($R(u) = \emptyset$): метрика = 1.0 (правильно предсказали отсутствие прочитанных книг)
- Если участник отправил хотя бы одну книгу: метрика = 0.0 (неправильно предложили книги, хотя ничего не прочитано)

### 4.2. Recall@20 (Полнота)

Показывает, какую долю скрытых прочитанных книг ($K$ штук) удалось найти и поместить в топ-20 рекомендованных (или во все позиции списка, если он короче 20 книг).

Для каждого пользователя $u$ рассчитывается:

$$\mathrm{Recall@20}(u) = \frac{|R(u)[:20] \cap GT(u)|}{K}$$

где $R(u)[:20]$ — первые 20 позиций ранжированного списка (или все позиции, если список короче 20).

Итоговая метрика является средним значением по всем пользователям:

$$\overline{\mathrm{Recall@20}} = \frac{1}{N} \sum_{j=1}^N \mathrm{Recall@20}(u_j)$$

**Обработка edge case (пустой solution):** Если пользователь ничего не прочитал в тестовом периоде ($K = 0$), то стандартные формулы дают деление на ноль. Для таких случаев применяется специальная логика:
- Если участник отправил пустой список ($R(u) = \emptyset$): метрика = 1.0 (правильно предсказали отсутствие прочитанных книг)
- Если участник отправил хотя бы одну книгу: метрика = 0.0 (неправильно предложили книги, хотя ничего не прочитано)

**Обозначения:**
- $N$ — количество пользователей в выборке.
- $u$ — конкретный пользователь.
- $K$ — количество скрытых прочитанных книг.
- $GT(u)$ — множество из $K$ скрытых книг, которые пользователь $u$ реально прочитал.
- $R(u)$ — ваш ранжированный список книг для пользователя $u$ (до 20 книг, или все доступные кандидаты, если их меньше 20).
- $rel(i)$ — индикатор релевантности: равен 1, если книга на $i$-й позиции вашего списка принадлежит $GT(u)$, и 0 в противном случае.

### 4.3. Итоговый балл (Score)

Финальный балл вычисляется как взвешенная сумма двух метрик. **Чем выше Score, тем лучше результат.**

$$\mathrm{Score} = 0.6 \cdot \overline{\mathrm{NDCG@20}} + 0.4 \cdot \overline{\mathrm{Recall@20}}$$

Лидерборд сортируется по убыванию значения Score.

---

## 5. Условия соревнования

### 5.1. Проверка и лидерборд

- **Публичный лидерборд (Public):** Рассчитывается на видимой части тестовых данных. Результат обновляется после каждой успешной отправки решения.
- **Приватный лидерборд (Private):** Рассчитывается на скрытой части тестовых данных. Финальные результаты соревнования определяются исключительно по этому лидерборду.
- **Лимиты:** Устанавливается ограничение на количество отправок в сутки и на весь этап. Точные значения будут указаны на странице соревнования.

### 5.2. Ограничения

- Запрещено использовать любые внешние данные и предварительно обученные модели.
- Ранжировать разрешено только книги из предоставленного для каждого пользователя пула кандидатов.
- В `train.csv` присутствуют временные метки (`timestamp`) для анализа истории взаимодействий пользователей. Однако в `candidates.csv` временные метки не предоставляются, что требует использования других признаков для ранжирования и усложняет задачу.

### 5.3. Честная игра

- Каждый участник или команда может использовать только одну учётную запись.
- Организаторы оставляют за собой право запросить код лучших решений для проверки на воспроизводимость и соответствие правилам. В случае расхождений результатов или нарушения правил решение может быть аннулировано.
- Любые попытки обойти правила, использовать уязвимости платформы или данные из будущих этапов ведут к дисквалификации.
